Mean_distance=mean(distance,na.rm = TRUE),
Stdev_distance=sd(distance,na.rm = TRUE))
program_admitted_location %>% group_by(schoolname) %>%
summarise(Mean_cutoff=mean(cutoff),
Stdev_cutoff=sd(cutoff),
Mean_quality=mean(quality),
Stdev_quality=sd(quality),
Mean_distance=mean(distance,na.rm = TRUE),
Stdev_distance=sd(distance,na.rm = TRUE)) %>% head()
program_admitted_location %>% group_by(schoolname) %>%
summarise(Mean_cutoff=mean(cutoff),
Stdev_cutoff=sd(cutoff),
Mean_quality=mean(quality),
Stdev_quality=sd(quality),
Mean_distance=mean(distance,na.rm = TRUE),
Stdev_distance=sd(distance,na.rm = TRUE)) %>% head()
program_admitted_location %>% group_by(Program) %>%
summarise(Mean_cutoff=mean(cutoff),
Stdev_cutoff=sd(cutoff),
Mean_quality=mean(quality),
Stdev_quality=sd(quality),
Mean_distance=mean(distance,na.rm = TRUE),
Stdev_distance=sd(distance,na.rm = TRUE)) %>%  head()
program_admitted_location %>% mutate(quantile=case_when(
score<quantile(score,0.1)~1,
score>=quantile(score,0.1) & score<quantile(score,0.2)~2,
score>=quantile(score,0.2) & score<quantile(score,0.3)~3,
score>=quantile(score,0.3) & score<quantile(score,0.4)~4,
score>=quantile(score,0.4) & score<quantile(score,0.5)~5,
score>=quantile(score,0.5) & score<quantile(score,0.6)~6,
score>=quantile(score,0.6) & score<quantile(score,0.7)~7,
score>=quantile(score,0.7) & score<quantile(score,0.8)~8,
score>=quantile(score,0.8) & score<quantile(score,0.9)~9,
score>=quantile(score,0.9) ~10,
)) %>% group_by(quantile) %>%  summarise(Mean_cutoff=mean(cutoff),
Stdev_cutoff=sd(cutoff),
Mean_quality=mean(quality),
Stdev_quality=sd(quality),
Mean_distance=mean(distance,na.rm = TRUE),
Stdev_distance=sd(distance,na.rm = TRUE)) %>%       head()
program_admitted_location %>% mutate(quantile=case_when(
score<quantile(score,0.1)~1,
score>=quantile(score,0.1) & score<quantile(score,0.2)~2,
score>=quantile(score,0.2) & score<quantile(score,0.3)~3,
score>=quantile(score,0.3) & score<quantile(score,0.4)~4,
score>=quantile(score,0.4) & score<quantile(score,0.5)~5,
score>=quantile(score,0.5) & score<quantile(score,0.6)~6,
score>=quantile(score,0.6) & score<quantile(score,0.7)~7,
score>=quantile(score,0.7) & score<quantile(score,0.8)~8,
score>=quantile(score,0.8) & score<quantile(score,0.9)~9,
score>=quantile(score,0.9) ~10,
)) %>% group_by(quantile) %>%  summarise(Mean_cutoff=mean(cutoff),
Stdev_cutoff=sd(cutoff),
Mean_quality=mean(quality),
Stdev_quality=sd(quality),
Mean_distance=mean(distance,na.rm = TRUE),
Stdev_distance=sd(distance,na.rm = TRUE)) %>%       head()
program_admitted_location %>% mutate(quantile=case_when(
score<quantile(score,0.1)~1,
score>=quantile(score,0.1) & score<quantile(score,0.2)~2,
score>=quantile(score,0.2) & score<quantile(score,0.3)~3,
score>=quantile(score,0.3) & score<quantile(score,0.4)~4,
score>=quantile(score,0.4) & score<quantile(score,0.5)~5,
score>=quantile(score,0.5) & score<quantile(score,0.6)~6,
score>=quantile(score,0.6) & score<quantile(score,0.7)~7,
score>=quantile(score,0.7) & score<quantile(score,0.8)~8,
score>=quantile(score,0.8) & score<quantile(score,0.9)~9,
score>=quantile(score,0.9) ~10,
)) %>% group_by(quantile) %>%  summarise(Mean_cutoff=mean(cutoff),
Stdev_cutoff=sd(cutoff),
Mean_quality=mean(quality),
Stdev_quality=sd(quality),
Mean_distance=mean(distance,na.rm = TRUE),
Stdev_distance=sd(distance,na.rm = TRUE))
set.seed(123)
# X1
X1 <- runif(10000,min=1,max=3)
# X2
X2 <- rgamma(10000,shape = 3,scale = 2)
# X3
X3 <- rbinom(10000,size=1,prob = 0.3)
# Error term
error <- rnorm(10000,mean=2,sd=1)
# Create Y and Ydum
# Y
par <- c(0.5,1.2,-0.9,0.1)
Y <- par[1] + par[2]*X1 + par[3]*X2 + par[4]*X3 + error
# Ydum
Ydum <- as.numeric(Y>mean(Y))
# Correlation Y and X1. How different is it from 1.2?
cor(Y,X1)
# answer: the result is 0.21, Being Y a linear function of X1 we would have expected the correlation
# to be larger.
# Calculate the coefficients
# For validation: summary(lm(Y~X1+X2+X3))
regressors <- as.matrix(t(rbind(rep(1,10000),X1,X2,X3)),ncol=4)
betas <-inv(t(regressors)%*%regressors)%*%(t(regressors)%*%Y)
# betas are the coefficients for the OLS estimation
resids <- Y-regressors%*%betas
sigma_2 <- as.numeric(t(resids)%*%resids/(10000-4))
var_cov_matrix <- sigma_2*inv(t(regressors)%*%regressors)
std_errors <- sqrt(diag(var_cov_matrix))
# std_errors are the standard errors of the coefficients
coef_stderrors <- cbind(betas,std_errors)
colnames(coef_stderrors) <- c("Coefs","Std. Errors")
print(coef_stderrors) # Answer
# Correlation Y and X1. How different is it from 1.2?
# answer: the result is 0.21, Being Y a linear function of X1 we would have expected the correlation to be larger.
cor(Y,X1)
# Calculate the coefficients
regressors <- as.matrix(t(rbind(rep(1,10000),X1,X2,X3)),ncol=4)
betas <-inv(t(regressors)%*%regressors)%*%(t(regressors)%*%Y)
# betas are the coefficients for the OLS estimation
resids <- Y-regressors%*%betas
sigma_2 <- as.numeric(t(resids)%*%resids/(10000-4))
var_cov_matrix <- sigma_2*inv(t(regressors)%*%regressors)
std_errors <- sqrt(diag(var_cov_matrix))
# std_errors are the standard errors of the coefficients
coef_stderrors <- cbind(betas,std_errors)
colnames(coef_stderrors) <- c("Coefs","Std. Errors")
print(coef_stderrors) # Answer
# The linear probability model can be estimated by OLS
# Function:
linear_prob_model <- function(Y,regressors){
betas <-inv(t(regressors)%*%regressors)%*%(t(regressors)%*%Y)
resids <- Y-regressors%*%betas
sigma_2 <- as.numeric(t(resids)%*%resids/(nrow(regressors)-ncol(regressors)))
var_cov_matrix <- sigma_2*inv(t(regressors)%*%regressors)
std_errors <- sqrt(diag(var_cov_matrix))
coef_stderrors <- cbind(betas,std_errors)
colnames(coef_stderrors) <- c("Coefs","Std. Errors")
return(coef_stderrors) # Answer
}
# Estimation
regressors <- as.matrix(t(rbind(rep(1,10000),X1,X2,X3)),ncol=4)
# Results of the linear probability model (Coefs and regressors)
results_lpm <- linear_prob_model(Ydum,regressors)
####   7.b  Probit ----
# Probit function
probit_likelihood = function(coefs,x1,x2,x3,y)
{
xbeta           = coefs[1] + coefs[2]*x1 + coefs[3]*x2 + coefs[4]*x3
pr              = pnorm(xbeta)
pr[pr>0.999999] = 0.999999
pr[pr<0.000001] = 0.000001
like            = y*log(pr) + (1-y)*log(1-pr)
return(-sum(like))
}
### Estimation
# Trying random starting points to check convergence ----
# tries = 100
# out_probit = mat.or.vec(tries,4)
# for (i in 1:tries)
# {
#   start    = runif(4,-10,10)
#   #res      = optim(start,fn=flike,method="BFGS",control=list(trace=6,REPORT=1,maxit=1000),x1=x1,x2=x2,x3=x3,yvar=yvar)
#   res      = optim(start,fn=probit_likelihood,method="BFGS",control=list(trace=6,maxit=1000),x1=X1,x2=X2,x3=X3,y=Ydum)
#   out_probit[i,] = res$par
# }
# # result: Using random starting points between -10 and 10 yields a wide range of parameter estimates
# Result Probit----
start = runif(4)
res_probit = optim(start,fn=probit_likelihood,method="BFGS",control=list(trace=6,REPORT=1,maxit=10000),x1=X1,x2=X2,x3=X3,y=Ydum,hessian=TRUE)
fisher_info_probit = solve(res_probit$hessian)
prop_sigma_probit  = sqrt(diag(fisher_info_probit))
####   7.c  Logit ----
# Logit function
logit_likelihood = function(y,x1,x2,x3,coefs)
{
xbeta           = coefs[1] + coefs[2]*x1 + coefs[3]*x2 + coefs[4]*x3
pr              = exp(xbeta)/(1+exp(xbeta))
pr[pr>0.999999] = 0.999999
pr[pr<0.000001] = 0.000001
like           = y*log(pr) + (1-y)*log(1-pr)
return(-sum(like))
}
### Estimation
# Trying random starting points to check convergence ----
# tries = 100
# out_logit = mat.or.vec(tries,4)
# for (i in 1:tries)
# {
#   start    = runif(4,-10,10)
#   res_logit      = optim(start,fn=logit_likelihood,method="BFGS",control=list(trace=6,maxit=1000),x1=X1,x2=X2,x3=X3,y=Ydum)
#   out_logit[i,] = res_logit$par
# }
# # result: Using random starting points between -10 and 10 yields a wide range of parameter estimates
# Result Logit ----
start = runif(4)
res_logit = optim(start,fn=logit_likelihood,method="BFGS",control=list(trace=6,REPORT=1,maxit=10000),
x1=X1,x2=X2,x3=X3,y=Ydum,hessian=TRUE)
fisher_info_logit = solve(res_logit$hessian)
prop_sigma_logit  = sqrt(diag(fisher_info_logit))
# Final Results ----
results = cbind(par,results_lpm[,1],results_lpm[,2],
res_probit$par,prop_sigma_probit,res_logit$par,prop_sigma_logit)
colnames(results) = c("True parameter","LPM: est","LPM :se","Probit: est","Probit: :se",
"Logit: est","Logit: :se")
results
# The linear probability model can be estimated by OLS
# Function:
linear_prob_model <- function(Y,regressors){
betas <-inv(t(regressors)%*%regressors)%*%(t(regressors)%*%Y)
resids <- Y-regressors%*%betas
sigma_2 <- as.numeric(t(resids)%*%resids/(nrow(regressors)-ncol(regressors)))
var_cov_matrix <- sigma_2*inv(t(regressors)%*%regressors)
std_errors <- sqrt(diag(var_cov_matrix))
coef_stderrors <- cbind(betas,std_errors)
colnames(coef_stderrors) <- c("Coefs","Std. Errors")
return(coef_stderrors) # Answer
}
# Estimation
regressors <- as.matrix(t(rbind(rep(1,10000),X1,X2,X3)),ncol=4)
# Results of the linear probability model (Coefs and regressors)
results_lpm <- linear_prob_model(Ydum,regressors)
####   7.b  Probit ----
# Probit function
probit_likelihood = function(coefs,x1,x2,x3,y)
{
xbeta           = coefs[1] + coefs[2]*x1 + coefs[3]*x2 + coefs[4]*x3
pr              = pnorm(xbeta)
pr[pr>0.999999] = 0.999999
pr[pr<0.000001] = 0.000001
like            = y*log(pr) + (1-y)*log(1-pr)
return(-sum(like))
}
### Estimation
# Trying random starting points to check convergence ----
# tries = 100
# out_probit = mat.or.vec(tries,4)
# for (i in 1:tries)
# {
#   start    = runif(4,-10,10)
#   #res      = optim(start,fn=flike,method="BFGS",control=list(trace=6,REPORT=1,maxit=1000),x1=x1,x2=x2,x3=x3,yvar=yvar)
#   res      = optim(start,fn=probit_likelihood,method="BFGS",control=list(trace=6,maxit=1000),x1=X1,x2=X2,x3=X3,y=Ydum)
#   out_probit[i,] = res$par
# }
# # result: Using random starting points between -10 and 10 yields a wide range of parameter estimates
# Result Probit----
start = runif(4)
res_probit = optim(start,fn=probit_likelihood,method="BFGS",control=list(trace=6,REPORT=0,maxit=10000),x1=X1,x2=X2,x3=X3,y=Ydum,hessian=TRUE)
# The linear probability model can be estimated by OLS
# Function:
linear_prob_model <- function(Y,regressors){
betas <-inv(t(regressors)%*%regressors)%*%(t(regressors)%*%Y)
resids <- Y-regressors%*%betas
sigma_2 <- as.numeric(t(resids)%*%resids/(nrow(regressors)-ncol(regressors)))
var_cov_matrix <- sigma_2*inv(t(regressors)%*%regressors)
std_errors <- sqrt(diag(var_cov_matrix))
coef_stderrors <- cbind(betas,std_errors)
colnames(coef_stderrors) <- c("Coefs","Std. Errors")
return(coef_stderrors) # Answer
}
# Estimation
regressors <- as.matrix(t(rbind(rep(1,10000),X1,X2,X3)),ncol=4)
# Results of the linear probability model (Coefs and regressors)
results_lpm <- linear_prob_model(Ydum,regressors)
####   7.b  Probit ----
# Probit function
probit_likelihood = function(coefs,x1,x2,x3,y)
{
xbeta           = coefs[1] + coefs[2]*x1 + coefs[3]*x2 + coefs[4]*x3
pr              = pnorm(xbeta)
pr[pr>0.999999] = 0.999999
pr[pr<0.000001] = 0.000001
like            = y*log(pr) + (1-y)*log(1-pr)
return(-sum(like))
}
### Estimation
# Trying random starting points to check convergence ----
# tries = 100
# out_probit = mat.or.vec(tries,4)
# for (i in 1:tries)
# {
#   start    = runif(4,-10,10)
#   #res      = optim(start,fn=flike,method="BFGS",control=list(trace=6,REPORT=1,maxit=1000),x1=x1,x2=x2,x3=x3,yvar=yvar)
#   res      = optim(start,fn=probit_likelihood,method="BFGS",control=list(trace=6,maxit=1000),x1=X1,x2=X2,x3=X3,y=Ydum)
#   out_probit[i,] = res$par
# }
# # result: Using random starting points between -10 and 10 yields a wide range of parameter estimates
# Result Probit----
start = runif(4)
res_probit = optim(start,fn=probit_likelihood,method="BFGS",control=list(trace=6,REPORT=10,maxit=10000),x1=X1,x2=X2,x3=X3,y=Ydum,hessian=TRUE)
fisher_info_probit = solve(res_probit$hessian)
prop_sigma_probit  = sqrt(diag(fisher_info_probit))
####   7.c  Logit ----
# Logit function
logit_likelihood = function(y,x1,x2,x3,coefs)
{
xbeta           = coefs[1] + coefs[2]*x1 + coefs[3]*x2 + coefs[4]*x3
pr              = exp(xbeta)/(1+exp(xbeta))
pr[pr>0.999999] = 0.999999
pr[pr<0.000001] = 0.000001
like           = y*log(pr) + (1-y)*log(1-pr)
return(-sum(like))
}
### Estimation
# Trying random starting points to check convergence ----
# tries = 100
# out_logit = mat.or.vec(tries,4)
# for (i in 1:tries)
# {
#   start    = runif(4,-10,10)
#   res_logit      = optim(start,fn=logit_likelihood,method="BFGS",control=list(trace=6,maxit=1000),x1=X1,x2=X2,x3=X3,y=Ydum)
#   out_logit[i,] = res_logit$par
# }
# # result: Using random starting points between -10 and 10 yields a wide range of parameter estimates
# Result Logit ----
start = runif(4)
res_logit = optim(start,fn=logit_likelihood,method="BFGS",control=list(trace=6,REPORT=1,maxit=10000),x1=X1,x2=X2,x3=X3,y=Ydum,hessian=TRUE)
fisher_info_logit = solve(res_logit$hessian)
prop_sigma_logit  = sqrt(diag(fisher_info_logit))
# Final Results ----
results = cbind(par,results_lpm[,1],results_lpm[,2],
res_probit$par,prop_sigma_probit,res_logit$par,prop_sigma_logit)
colnames(results) = c("True parameter","LPM: est","LPM :se","Probit: est","Probit: :se",
"Logit: est","Logit: :se")
results
# The linear probability model can be estimated by OLS
# Function:
linear_prob_model <- function(Y,regressors){
betas <-inv(t(regressors)%*%regressors)%*%(t(regressors)%*%Y)
resids <- Y-regressors%*%betas
sigma_2 <- as.numeric(t(resids)%*%resids/(nrow(regressors)-ncol(regressors)))
var_cov_matrix <- sigma_2*inv(t(regressors)%*%regressors)
std_errors <- sqrt(diag(var_cov_matrix))
coef_stderrors <- cbind(betas,std_errors)
colnames(coef_stderrors) <- c("Coefs","Std. Errors")
return(coef_stderrors) # Answer
}
# Estimation
regressors <- as.matrix(t(rbind(rep(1,10000),X1,X2,X3)),ncol=4)
# Results of the linear probability model (Coefs and regressors)
results_lpm <- linear_prob_model(Ydum,regressors)
####   7.b  Probit ----
# Probit function
probit_likelihood = function(coefs,x1,x2,x3,y)
{
xbeta           = coefs[1] + coefs[2]*x1 + coefs[3]*x2 + coefs[4]*x3
pr              = pnorm(xbeta)
pr[pr>0.999999] = 0.999999
pr[pr<0.000001] = 0.000001
like            = y*log(pr) + (1-y)*log(1-pr)
return(-sum(like))
}
### Estimation
# Trying random starting points to check convergence ----
# tries = 100
# out_probit = mat.or.vec(tries,4)
# for (i in 1:tries)
# {
#   start    = runif(4,-10,10)
#   #res      = optim(start,fn=flike,method="BFGS",control=list(trace=6,REPORT=1,maxit=1000),x1=x1,x2=x2,x3=x3,yvar=yvar)
#   res      = optim(start,fn=probit_likelihood,method="BFGS",control=list(trace=6,maxit=1000),x1=X1,x2=X2,x3=X3,y=Ydum)
#   out_probit[i,] = res$par
# }
# # result: Using random starting points between -10 and 10 yields a wide range of parameter estimates
# Result Probit----
start = runif(4)
res_probit = optim(start,fn=probit_likelihood,method="BFGS",control=list(trace=6,REPORT=10,maxit=10000),x1=X1,x2=X2,x3=X3,y=Ydum,hessian=TRUE)
fisher_info_probit = solve(res_probit$hessian)
prop_sigma_probit  = sqrt(diag(fisher_info_probit))
####   7.c  Logit ----
# Logit function
logit_likelihood = function(y,x1,x2,x3,coefs)
{
xbeta           = coefs[1] + coefs[2]*x1 + coefs[3]*x2 + coefs[4]*x3
pr              = exp(xbeta)/(1+exp(xbeta))
pr[pr>0.999999] = 0.999999
pr[pr<0.000001] = 0.000001
like           = y*log(pr) + (1-y)*log(1-pr)
return(-sum(like))
}
### Estimation
# Trying random starting points to check convergence ----
# tries = 100
# out_logit = mat.or.vec(tries,4)
# for (i in 1:tries)
# {
#   start    = runif(4,-10,10)
#   res_logit      = optim(start,fn=logit_likelihood,method="BFGS",control=list(trace=6,maxit=1000),x1=X1,x2=X2,x3=X3,y=Ydum)
#   out_logit[i,] = res_logit$par
# }
# # result: Using random starting points between -10 and 10 yields a wide range of parameter estimates
# Result Logit ----
start = runif(4)
res_logit = optim(start,fn=logit_likelihood,method="BFGS",control=list(trace=6,REPORT=10,maxit=10000),x1=X1,x2=X2,x3=X3,y=Ydum,hessian=TRUE)
fisher_info_logit = solve(res_logit$hessian)
prop_sigma_logit  = sqrt(diag(fisher_info_logit))
# Final Results ----
results = cbind(par,results_lpm[,1],results_lpm[,2],
res_probit$par,prop_sigma_probit,res_logit$par,prop_sigma_logit)
colnames(results) = c("True parameter","LPM: est","LPM :se","Probit: est","Probit: :se",
"Logit: est","Logit: :se")
results
# The linear probability model can be estimated by OLS
# Function:
linear_prob_model <- function(Y,regressors){
betas <-inv(t(regressors)%*%regressors)%*%(t(regressors)%*%Y)
resids <- Y-regressors%*%betas
sigma_2 <- as.numeric(t(resids)%*%resids/(nrow(regressors)-ncol(regressors)))
var_cov_matrix <- sigma_2*inv(t(regressors)%*%regressors)
std_errors <- sqrt(diag(var_cov_matrix))
coef_stderrors <- cbind(betas,std_errors)
colnames(coef_stderrors) <- c("Coefs","Std. Errors")
return(coef_stderrors) # Answer
}
# Estimation
regressors <- as.matrix(t(rbind(rep(1,10000),X1,X2,X3)),ncol=4)
# Results of the linear probability model (Coefs and regressors)
results_lpm <- linear_prob_model(Ydum,regressors)
####   7.b  Probit ----
# Probit function
probit_likelihood = function(coefs,x1,x2,x3,y)
{
xbeta           = coefs[1] + coefs[2]*x1 + coefs[3]*x2 + coefs[4]*x3
pr              = pnorm(xbeta)
pr[pr>0.999999] = 0.999999
pr[pr<0.000001] = 0.000001
like            = y*log(pr) + (1-y)*log(1-pr)
return(-sum(like))
}
### Estimation
# Trying random starting points to check convergence ----
# tries = 100
# out_probit = mat.or.vec(tries,4)
# for (i in 1:tries)
# {
#   start    = runif(4,-10,10)
#   #res      = optim(start,fn=flike,method="BFGS",control=list(trace=6,REPORT=1,maxit=1000),x1=x1,x2=x2,x3=x3,yvar=yvar)
#   res      = optim(start,fn=probit_likelihood,method="BFGS",control=list(trace=6,maxit=1000),x1=X1,x2=X2,x3=X3,y=Ydum)
#   out_probit[i,] = res$par
# }
# # result: Using random starting points between -10 and 10 yields a wide range of parameter estimates
# Result Probit----
start = runif(4)
res_probit = optim(start,fn=probit_likelihood,method="BFGS",control=list(trace=6,REPORT=10,maxit=10000),x1=X1,x2=X2,x3=X3,y=Ydum,hessian=TRUE)
fisher_info_probit = solve(res_probit$hessian)
prop_sigma_probit  = sqrt(diag(fisher_info_probit))
####   7.c  Logit ----
# Logit function
logit_likelihood = function(y,x1,x2,x3,coefs)
{
xbeta           = coefs[1] + coefs[2]*x1 + coefs[3]*x2 + coefs[4]*x3
pr              = exp(xbeta)/(1+exp(xbeta))
pr[pr>0.999999] = 0.999999
pr[pr<0.000001] = 0.000001
like           = y*log(pr) + (1-y)*log(1-pr)
return(-sum(like))
}
### Estimation
# Trying random starting points to check convergence ----
# tries = 100
# out_logit = mat.or.vec(tries,4)
# for (i in 1:tries)
# {
#   start    = runif(4,-10,10)
#   res_logit      = optim(start,fn=logit_likelihood,method="BFGS",control=list(trace=6,maxit=1000),x1=X1,x2=X2,x3=X3,y=Ydum)
#   out_logit[i,] = res_logit$par
# }
# # result: Using random starting points between -10 and 10 yields a wide range of parameter estimates
# Result Logit ----
start = runif(4)
res_logit = optim(start,fn=logit_likelihood,method="BFGS",control=list(trace=6,REPORT=10,maxit=10000),x1=X1,x2=X2,x3=X3,y=Ydum,hessian=TRUE)
fisher_info_logit = solve(res_logit$hessian)
prop_sigma_logit  = sqrt(diag(fisher_info_logit))
# Final Results ----
results = cbind(par,results_lpm[,1],results_lpm[,2],
res_probit$par,prop_sigma_probit,res_logit$par,prop_sigma_logit)
colnames(results) = c("True parameter","LPM: est","LPM :se","Probit: est","Probit: :se",
"Logit: est","Logit: :se")
results
# Answer
# 1. The LPM, which is the only comparable model in terms of coefficients, produces estimates
# really different from the true parameters. At least they are all un the correct direction.
# Using a t-test with 95% confidence, the intercept, X1 and X2 are significant. This is not the case for X3.
Significance_lpm <- abs(results_lpm[,1]/results_lpm[,2])>1.96
Significance_lpm
# 2. The Probit model coefficients are not directly comparable with the true parameters.
# We can observe that the sign of the estimates is correct for all but X3.
# Using a t-test with 95% confidence, the intercept, X1 and X2 are significant. This is not the case for X3.
Significance_probit <-abs(res_probit$par/prop_sigma_probit)>1.96
# 2. The Probit model coefficients are not directly comparable with the true parameters.
# We can observe that the sign of the estimates is correct for all but X3.
# Using a t-test with 95% confidence, the intercept, X1 and X2 are significant. This is not the case for X3.
Significance_probit <-abs(res_probit$par/prop_sigma_probit)>1.96
Significance_probit
# 3. The Logit model coefficients are not directly comparable with the true parameters.
# We can observe that the sign of the estimates is correct for all but X3.
# Using a t-test with 95% confidence, the intercept, X1 and X2 are significant. This is not the case for X3.
Significance_logit <- abs(res_logit$par/prop_sigma_logit)>1.96
Significance_logit
#### 8.1. Probit average marginal effects
Xbeta_probit <- regressors %*% as.matrix(res_probit$par)
mgl_effects_probit <- pnorm(Xbeta_probit)%*% t(as.matrix(res_probit$par))
mean_mgleff_probit <- colMeans(mgl_effects_probit)
sd_mgleff_probit <- apply(mgl_effects_probit,2,sd)
#### 8.2. Logit average marginal effects
Xbeta_logit <- regressors %*% as.matrix(res_logit$par)
mgl_effects_logit <- (plogis(Xbeta_logit)*(1-plogis(Xbeta_logit)))%*%t(as.matrix(res_logit$par))
mean_mgleff_logit <- colMeans(mgl_effects_logit)
sd_mgleff_logit <- apply(mgl_effects_logit,2,sd)
## Answers
avg_mgleffects <- cbind(mean_mgleff_probit,sd_mgleff_probit,mean_mgleff_logit,sd_mgleff_logit)
avg_mgleffects <- avg_mgleffects[-1,]
colnames(avg_mgleffects) <- c("Probit: Avg Mgl Eff","Probit: SD of Mgl Eff",
"Logit: Avg Mgl Eff","Logit: SD of Mgl Eff")
rownames(avg_mgleffects) <- c("X1","X2","X3")
avg_mgleffects # Answer
# The average marginal effects are, in general, larger in the probit model than in the logit model.
# The same is true for the standard errors. In both models, X1 has the largest marginal effect.
tinytex::install_tinytex()
